# calls the llm API, shows a nice spinner while it's running 
# called without a subshell to stay in the widget context, returns the answer in $generated_text variable
emulate -L zsh
local intro="$1"
local prompt="$2"
local progress_text="$3"

local response_file=$(mktemp)

local data=$(jq -nr --arg prompt "$prompt" --arg intro "$intro" '{
  "messages":[
    {"role": "system", "content": $intro},
    {"role": "user", "content": $prompt}
  ],
  "model":"gpt-3.5-turbo",
  "max_tokens":256,
  "temperature":0
}')

# Read the response from file
# Todo: avoid using temp files
set +m
if command -v curl &> /dev/null; then
  { curl -s -X POST -H "Content-Type: application/json" -H "Authorization: Bearer $OPENAI_API_KEY" -d "$data" https://api.openai.com/v1/chat/completions > "$response_file" } &>/dev/null &
else
  { wget -qO- --header="Content-Type: application/json" --header="Authorization: Bearer $OPENAI_API_KEY" --post-data="$data" https://api.openai.com/v1/chat/completions > "$response_file" } &>/dev/null &
fi
local pid=$!

# Display a spinner while the API request is running in the background
local spinner=("⠋" "⠙" "⠹" "⠸" "⠼" "⠴" "⠦" "⠧" "⠇" "⠏")
local i
while true; do
  for i in "${spinner[@]}"; do
    if ! kill -0 $pid 2> /dev/null; then
      break 2
    fi

    zle -R "$i $progress_text"
    sleep 0.1
  done
done

wait $pid
if [ $? -ne 0 ]; then
  zle -M "Error: API request failed"
  return 1
fi

local error=$(jq -r '.error.message' < $response_file)
generated_text=$(jq -r '.choices[0].message.content' < $response_file | tr '\n' '\r' | sed -e $'s/^[ \r`]*//; s/[ \r`]*$//' | tr '\r' '\n')

# explicit rm invocation to avoid user shell overrides
command rm "$response_file"


if [ $? -ne 0 ]; then
  zle -M "Error: Invalid API response format"
  return 1
fi

if [[ -n "$error" && "$error" != "null" ]]; then
  zle -M "API error: $error"
  return 1
fi

